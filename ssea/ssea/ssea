#!/bin/env python2.7
# encoding: utf-8
'''
 -- Sample Set Enrichment Analysis (SSEA) --

Assessment of enrichment in a ranked list of quantitative measurements 

@author:     mkiyer
@author:     yniknafs
        
@copyright:  2013 Michigan Center for Translational Pathology. All rights reserved.
        
@license:    GPL2

@contact:    mkiyer@umich.edu
@deffield    updated: Updated
'''
# set matplotlib backend
import matplotlib
matplotlib.use('Agg')

import sys
import os
import shutil
import argparse
import logging
import json
import matplotlib.pyplot as plt
import numpy as np
from multiprocessing import Process, JoinableQueue

__all__ = []

DEBUG = 1
TESTRUN = 0
PROFILE = 0

# setup path to web files
import ssea as _ssea
SRC_WEB_PATH = os.path.join(_ssea.__path__[0], 'web')

# local imports
from ssea import __version__, __date__, __updated__
from ssea.config import Config, DETAILS_DIR
from ssea.base import SampleSet, WeightVector
from ssea.algo import ssea_run

# setup html template environment
from jinja2 import Environment, PackageLoader
env = Environment(loader=PackageLoader("ssea", "templates"),
                  extensions=["jinja2.ext.loopcontrols"])

# header fields for report
REPORT_FIELDS = ['name', 
                 'desc',
                 'sample_set_name',
                 'sample_set_desc',
                 'sample_set_size',
                 'es',
                 'nes',
                 'nominal_p_value',
                 'fdr_q_value',
                 'fwer_p_value',
                 'global_nes',
                 'global_fdr_q_value',
                 'rank_at_max',
                 'leading_edge_num_hits',
                 'leading_edge_frac_hits',
                 'sample_set_frac_in_leading_edge',
                 'null_set_frac_in_leading_edge',
                 'details']
REPORT_FIELD_MAP = dict((v,k) for k,v in enumerate(REPORT_FIELDS))

# matplotlib static figure
global_fig = plt.figure(0)

# null enrichment score histogram bins
NUM_BINS = 10000
BINS_NEG = np.linspace(-1.0, 0.0, num=NUM_BINS+1)
BINS_POS = np.linspace(0.0, 1.0, num=NUM_BINS+1)
BIN_CENTERS_NEG = (BINS_NEG[:-1] + BINS_NEG[1:]) / 2.
BIN_CENTERS_POS = (BINS_POS[:-1] + BINS_POS[1:]) / 2.

def compute_global_stats(sample_sets, es_hists_file, input_tsv_file,
                         output_tsv_file):
    # read es arrays
    npzfile = np.load(es_hists_file)
    hists_null_pos = npzfile['null_pos']
    hists_null_neg = npzfile['null_neg']
    hists_obs_pos = npzfile['obs_pos']
    hists_obs_neg = npzfile['obs_neg']
    npzfile.close()
    # compute totals
    hists_null_pos_counts = hists_null_pos.sum(axis=1)
    hists_null_neg_counts = hists_null_neg.sum(axis=1)
    hists_obs_pos_counts = hists_obs_pos.sum(axis=1)
    hists_obs_neg_counts = hists_obs_neg.sum(axis=1)
    # compute means
    hists_null_neg_means = np.fabs((hists_null_neg * BIN_CENTERS_NEG).sum(axis=1) / 
                                   hists_null_neg_counts)
    hists_null_pos_means = np.fabs((hists_null_pos * BIN_CENTERS_POS).sum(axis=1) / 
                                   hists_null_pos_counts)
    # map sample set names to indexes
    sample_set_ind_map = \
        dict((ss.name, i) for i,ss in enumerate(sample_sets))
    # read report
    fout = open(output_tsv_file, 'w')
    print >>fout, '\t'.join(REPORT_FIELDS)
    fin = open(input_tsv_file)
    fin.next()
    es_col_ind = REPORT_FIELD_MAP['es']
    sample_set_col_ind = REPORT_FIELD_MAP['sample_set_name']
    global_nes_ind = REPORT_FIELD_MAP['global_nes']
    global_qval_ind = REPORT_FIELD_MAP['global_fdr_q_value']
    for line in fin:
        # read result
        fields = line.strip().split('\t')
        sample_set_name = fields[sample_set_col_ind]
        i = sample_set_ind_map[sample_set_name]
        es = float(fields[es_col_ind])
        # compute global nes and fdr        
        if es < 0:
            global_nes = es / hists_null_neg_means[i]            
            es_bin = np.digitize((es,), BINS_NEG)
            n = hists_null_neg[i,:es_bin].sum() / float(hists_null_neg_counts[i])
            d = hists_obs_neg[i,:es_bin].sum() / float(hists_obs_neg_counts[i])
        else:
            global_nes = es / hists_null_pos_means[i]            
            es_bin = np.digitize((es,), BINS_POS) - 1
            n = hists_null_pos[i,es_bin:].sum() / float(hists_null_pos_counts[i])
            d = hists_obs_pos[i,es_bin:].sum() / float(hists_obs_pos_counts[i])
        global_fdr_qval = n / d
        fields[global_nes_ind] = str(global_nes)
        fields[global_qval_ind] = str(global_fdr_qval)
        print >>fout, '\t'.join(fields)
    fin.close()
    fout.close()

def format_result(name, desc, res):
    # calculate leading edge stats
    member_inds = (res.membership > 0).nonzero()[0]
    le_num_hits = sum(ind <= res.es_run_ind 
                      for ind in member_inds)
    le_num_misses = res.es_run_ind - le_num_hits
    num_misses = res.membership.shape[0] - len(res.sample_set)
    sample_set_frac_le = float(le_num_hits) / len(res.sample_set)
    null_set_frac_le = float(le_num_misses) / num_misses
    if res.es_run_ind == 0:
        le_frac_hits = 0.0
    else:
        le_frac_hits = float(le_num_hits) / res.es_run_ind
    # write result to text file            
    fields = [name, desc,
              res.sample_set.name, res.sample_set.desc, 
              len(res.sample_set), res.es, res.nes, res.pval, 
              res.qval, res.fwerp, 'NA', 'NA',
              res.es_run_ind,
              le_num_hits, le_frac_hits, 
              sample_set_frac_le,
              null_set_frac_le]
    return fields

def write_details(name, desc, res, config):
    '''
    name: string
    desc: string
    res: algo.SampleSetResult object
    config: config.Config object
    
    returns dict containing filenames written
    '''
    d = {}
    if config.create_plots:
        # create enrichment plot
        res.plot(plot_conf_int=config.plot_conf_int,
                 conf_int=config.conf_int, fig=global_fig)    
        # save plots
        eplot_png = '%s.%s.eplot.png' % (name, res.sample_set.name)
        eplot_pdf = '%s.%s.eplot.pdf' % (name, res.sample_set.name)
        global_fig.savefig(os.path.join(config.details_dir, eplot_png))
        global_fig.savefig(os.path.join(config.details_dir, eplot_pdf))
        # create null distribution plot
        res.plot_null_distribution(fig=global_fig)
        nplot_png = '%s.%s.null.png' % (name, res.sample_set.name)
        nplot_pdf = '%s.%s.null.pdf' % (name, res.sample_set.name)
        global_fig.savefig(os.path.join(config.details_dir, nplot_png))        
        global_fig.savefig(os.path.join(config.details_dir, nplot_pdf))
        d.update({'eplot_png': eplot_png,
                  'nplot_png': nplot_png})
        d.update({'eplot_pdf': eplot_pdf,
                  'nplot_pdf': nplot_pdf})
    # write detailed report
    details_rows = res.get_details_table()
    details_tsv = '%s.%s.tsv' % (name, res.sample_set.name)
    fp = open(os.path.join(config.details_dir, details_tsv), 'w')
    for fields in details_rows:
        print >>fp, '\t'.join(map(str,fields))
    fp.close()
    d['tsv'] = details_tsv
    # render to html
    if config.create_html:
        result_dict = dict(zip(REPORT_FIELDS, format_result(name, desc, res)))
        details_html = '%s.%s.html' % (name, res.sample_set.name)
        t = env.get_template('details.html')
        fp = open(os.path.join(config.details_dir, details_html), 'w')
        print >>fp, t.render(res=result_dict, 
                             files=d,
                             details=details_rows)
        fp.close()
        d['html'] = details_html
    return d

def ssea_weight_vector(weight_vec, sample_sets, config, outfileh, es_hists):
    '''
    Run SSEA on a single WeightVector object across a list of SampleSet 
    objects
    
    weight_vec: WeightVector object
    sample_sets: list of SampleSet objects
    config: Config object
    outfileh: open file object for writing results
    es_hists: dictionary of histogram data for ES distributions
    '''
    logging.info("\tName: %s" % (weight_vec.name))
    results = ssea_run(weight_vec.samples, 
                       weight_vec.weights, 
                       sample_sets, 
                       weight_method_miss=config.weight_miss,
                       weight_method_hit=config.weight_hit,
                       weight_const=config.weight_const,
                       perms=config.perms)
    name = weight_vec.name
    desc = ' '.join(weight_vec.metadata)
    for i,res in enumerate(results):
        # get output fields
        fields = format_result(name, desc, res) 
        # decide whether to create detailed report
        if res.qval <= config.fdr_qval_threshold:                
            details_dict = write_details(name, desc, res, config) 
            details_json = json.dumps(details_dict)
        else:
            details_json = json.dumps({})
        # output to text file
        fields.append(details_json)
        print >>outfileh, '\t'.join(map(str, fields))
        # update ES histograms
        if res.es <= 0:
            es_hists['null_neg'][i] += np.histogram(res.es_null, BINS_NEG)[0]
            es_hists['obs_neg'][i] += np.histogram(res.es, BINS_NEG)[0]
        if res.es >= 0:
            es_hists['null_pos'][i] += np.histogram(res.es_null, BINS_POS)[0]        
            es_hists['obs_pos'][i] += np.histogram(res.es, BINS_POS)[0]

def ssea_serial(weight_vec_iter, sample_sets, config, output_tsv_file,
                output_hist_file):
    '''
    main SSEA loop (single processor)
    '''
    # setup global ES histograms
    es_hists = {'null_pos': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float),
                'null_neg': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float),
                'obs_pos': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float),
                'obs_neg': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float)}
    # setup report file
    fp = open(output_tsv_file, 'w')
    print >>fp, '\t'.join(REPORT_FIELDS)
    for weight_vec in weight_vec_iter:
        ssea_weight_vector(weight_vec, sample_sets, config, fp, es_hists)
    fp.close()
    # save histograms to a file
    np.savez(output_hist_file, **es_hists)

def ssea_parallel(weight_vec_iter, sample_sets, config, output_tsv_file,
                  output_hist_file): 
    '''
    main SSEA loop (multiprocessing implementation)
    '''
    def worker(input_queue, sample_sets, config, tsv_file, hist_file):
        def queue_iter(q):
            while True:
                obj = q.get()
                if (obj is None):
                    break
                yield obj
                input_queue.task_done()
            input_queue.task_done()
        # initialize output file
        ssea_serial(queue_iter(input_queue), sample_sets, config, 
                    tsv_file, hist_file)
    # create temp directory
    tmp_dir = os.path.join(config.output_dir, "tmp")
    if not os.path.exists(tmp_dir):
        logging.debug("\tCreating tmp directory '%s'" % (tmp_dir))
        os.makedirs(tmp_dir)
    # create multiprocessing queue for passing data
    input_queue = JoinableQueue(maxsize=config.num_processors*3)
    # start worker processes
    procs = []
    worker_tsv_files = []
    worker_hist_files = []
    try:
        for i in xrange(config.num_processors):
            tsv_file = os.path.join(tmp_dir, "w%03d_report.tsv" % (i))
            worker_tsv_files.append(tsv_file)
            hist_file = os.path.join(tmp_dir, "w%03d_hists.npz" % (i))
            worker_hist_files.append(hist_file)
            args = (input_queue, sample_sets, config, tsv_file, hist_file) 
            p = Process(target=worker, args=args)
            p.start()
            procs.append(p)
        # parse weight vectors
        for weight_vec in weight_vec_iter:
            input_queue.put(weight_vec)
    finally:
        # stop workers
        for p in procs:
            input_queue.put(None)
        # close queue
        input_queue.close()
        input_queue.join()
        # join worker processes
        for p in procs:
            p.join()
    # merge workers
    logging.info("Merging %d worker results" % (config.num_processors))
    fp = open(output_tsv_file, 'w')
    print >>fp, '\t'.join(REPORT_FIELDS)
    # setup global ES histograms
    es_hists = {'null_pos': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float),
                'null_neg': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float),
                'obs_pos': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float),
                'obs_neg': np.zeros((len(sample_sets),NUM_BINS), dtype=np.float)}
    for i in xrange(config.num_processors):
        # merge report tsv
        tsv_file = worker_tsv_files[i]
        with open(tsv_file, 'r') as fin:
            fin.next() # skip header
            for line in fin:
                print >>fp, line.rstrip()
        # aggregate numpy arrays
        npzfile = np.load(worker_hist_files[i])
        es_hists['null_pos'] += npzfile['null_pos']
        es_hists['null_neg'] += npzfile['null_neg']
        es_hists['obs_pos'] += npzfile['obs_pos']
        es_hists['obs_neg'] += npzfile['obs_neg']
        npzfile.close()
    fp.close() 
    np.savez(output_hist_file, **es_hists)

def write_html_report(filename, config):
    def parse_report_as_dicts(filename):
        '''
        parses lines of the out.txt report file produced by SSEA and 
        generates dictionaries using the first line of the file
        containing the header fields
        '''
        fileh = open(filename, 'r')
        header_fields = fileh.next().strip().split('\t')
        details_ind = header_fields.index('details')
        for line in fileh:
            fields = line.strip().split('\t')
            fields[details_ind] = json.loads(fields[details_ind])
            yield dict(zip(header_fields, fields))
        fileh.close()
    report_html = 'out.html'
    t = env.get_template('report.html')
    fp = open(os.path.join(config.output_dir, report_html), 'w')
    print >>fp, t.render(name=config.name,
                         details_dir=DETAILS_DIR,
                         results=parse_report_as_dicts(filename))
    fp.close()

def ssea_main(config):
    # read sample sets
    logging.info("Reading sample sets")
    sample_sets = []
    for filename in config.smx_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smx(filename))
    for filename in config.smt_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smt(filename))
    logging.info("\tNumber of sample sets: %d" % (len(sample_sets)))
    filtered_sample_sets = []
    for sample_set in sample_sets:
        if ((config.sample_set_size_min > 0) and
            (len(sample_set.value) < config.sample_set_size_min)):
            logging.warning("\tsample set %s excluded because size %d < %d" %                              
                            (sample_set.name, len(sample_set.value), 
                             config.sample_set_size_min))
            continue        
        if ((config.sample_set_size_max > 0) and 
            (len(sample_set.value) > config.sample_set_size_max)):
            logging.warning("\tsample set %s excluded because size %d > %d" % 
                            (sample_set.name, len(sample_set.value), 
                             config.sample_set_size_max))
            continue
        logging.debug("\tsample set %s size %d" % (sample_set.name, len(sample_set)))
        filtered_sample_sets.append(sample_set)
    logging.info("\tNumber of filtered sample sets: %d" % (len(filtered_sample_sets)))
    sample_sets = filtered_sample_sets
    # setup output directory
    if not os.path.exists(config.output_dir):
        logging.info("Creating output directory '%s'" % 
                     (config.output_dir))
        os.makedirs(config.output_dir)
    if not os.path.exists(config.details_dir):
        logging.debug("\tCreating details directory '%s'" % 
                      (config.details_dir))
        os.makedirs(config.details_dir)
    # create temp directory
    tmp_dir = os.path.join(config.output_dir, "tmp")
    if not os.path.exists(tmp_dir):
        logging.debug("\tCreating tmp directory '%s'" % (tmp_dir))
        os.makedirs(tmp_dir)
    # parse weight matrix file
    # TODO: read metadata cols
    weight_vec_iter = WeightVector.parse_wmt(config.weight_matrix_file, 
                                             na_value=config.na_value,
                                             metadata_cols=2)
    # output files
    tmp_tsv_file = os.path.join(tmp_dir, 'out.tsv')
    es_hists_file = os.path.join(config.output_dir, "es_hists.npz")
    if config.num_processors > 1:
        logging.info("Running SSEA in parallel with %d processes" % 
                     (config.num_processors))
        ssea_parallel(weight_vec_iter, sample_sets, config,
                      tmp_tsv_file, es_hists_file)
    else:
        logging.info("Running SSEA in serial")
        ssea_serial(weight_vec_iter, sample_sets, config, 
                    tmp_tsv_file, es_hists_file)
    # use ES null distributions to compute global statistics
    # and produce a report
    report_tsv_file = os.path.join(config.output_dir, 'out.tsv')
    compute_global_stats(sample_sets, es_hists_file, tmp_tsv_file,
                         report_tsv_file)
    # create html report
    if config.create_html:
        logging.info("Writing HTML Report")
        write_html_report(report_tsv_file, config)
        # create directory for static web files (CSS, javascript, etc)
        web_dir = os.path.join(config.output_dir, 'web')
        if not os.path.exists(web_dir):
            logging.info("\tInstalling web files")
            shutil.copytree(SRC_WEB_PATH, web_dir)
    # free resources
    plt.close('all')
    # cleanup
    if os.path.exists(tmp_dir):
        shutil.rmtree(tmp_dir)    
    logging.info("Finished")

def main(argv=None):
    '''Command line options.'''    
    if argv is None:
        argv = sys.argv
    else:
        sys.argv.extend(argv)

    program_name = os.path.basename(sys.argv[0])
    program_version = "v%s" % __version__
    program_build_date = str(__updated__)
    program_version_message = '%s %s (%s)' % (program_name, program_version, program_build_date)
    program_shortdesc = __import__('__main__').__doc__.split("\n")[1]
    program_license = '''%s

  Created by mkiyer and yniknafs on %s.
  Copyright 2013 MCTP. All rights reserved.
  
  Licensed under the GPL
  http://www.gnu.org/licenses/gpl.html
  
  Distributed on an "AS IS" basis without warranties
  or conditions of any kind, either express or implied.

USAGE
''' % (program_shortdesc, str(__date__))

    # create instance of run configuration
    config = Config()
    config.version = program_version_message
    try:
        # Setup argument parser
        parser = argparse.ArgumentParser(description=program_license)
        # Add command line parameters
        config.get_argument_parser(parser)
        parser.add_argument("-v", "--verbose", dest="verbose", 
                            action="store_true", default=False, 
                            help="set verbosity level [default: %(default)s]")
        parser.add_argument('-V', '--version', action='version', 
                            version=program_version_message)
        # Process arguments
        args = parser.parse_args()
        # setup logging
        if DEBUG or (args.verbose > 0):
            level = logging.DEBUG
        else:
            level = logging.INFO
        logging.basicConfig(level=level,
                            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        # initialize configuration
        config.parse_args(parser, args)
        config.log()
        # run
        ssea_main(config)
    except KeyboardInterrupt:
        ### handle keyboard interrupt ###
        pass
#     except Exception, e:
#         pass
#         if DEBUG or TESTRUN:
#             raise(e)
#         indent = len(program_name) * " "
#         logging.error(program_name + ": " + repr(e) + "\n")
#         logging.error(indent + "  for help use --help")
#         return 2
    return 0

if __name__ == "__main__":
    if DEBUG:
        pass
    if TESTRUN:
        pass
        #import doctest
        #doctest.testmod()
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = '_profile.bin'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit(main())