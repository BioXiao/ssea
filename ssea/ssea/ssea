#!/bin/env python2.7
# encoding: utf-8
'''
 -- Sample Set Enrichment Analysis (SSEA) --

Assessment of enrichment in a ranked list of quantitative measurements 

@author:     mkiyer
@author:     yniknafs
        
@copyright:  2013 Michigan Center for Translational Pathology. All rights reserved.
        
@license:    GPL2

@contact:    mkiyer@umich.edu
@deffield    updated: Updated
'''
import sys
import os
import logging
import shutil
import subprocess

__all__ = []

PROFILE = 0

# local imports
from ssea.lib.config import Config 
from ssea.lib.base import SampleSet, chunk
from ssea.lib.countdata import BigCountMatrix
from ssea.lib.algo import ssea_map, ssea_reduce, ssea_serial

CLUSTER_WORKER_FILE = 'workers.txt'
CLUSTER_MAP_SCRIPT = 'map.pbs'
CLUSTER_REDUCE_SCRIPT = 'reduce.pbs'
CLUSTER_MAPREDUCE_SCRIPT = 'run.pbs'

# cluster walltime computation constants
SECS_PER_ITER_MAP = 1.5e-7
SECS_PER_ITER_REDUCE = 0.01
HOURS_PER_SEC = (1.0 / 3600.0)
PADDING_HOURS = 2.0

def read_sample_sets(config):
    '''
    config: ssea.lib.Config object with SSEA options specified
    
    returns list of SampleSet objects
    '''
    # read sample sets
    logging.info("Reading sample sets")
    sample_sets = []
    for filename in config.smx_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smx(filename))
    for filename in config.smt_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smt(filename))
    logging.info("\tNumber of sample sets: %d" % (len(sample_sets)))
    filtered_sample_sets = []
    for sample_set in sample_sets:
        if ((config.smin > 0) and (len(sample_set) < config.smin)):
            logging.warning("\tsample set %s excluded because size %d < %d" %                              
                            (sample_set.name, len(sample_set), config.smin))
            continue        
        if ((config.smax > 0) and (len(sample_set) > config.smax)):
            logging.warning("\tsample set %s excluded because size %d > %d" % 
                            (sample_set.name, len(sample_set), config.smax))
            continue
        logging.debug("\tsample set %s size %d" % 
                      (sample_set.name, len(sample_set)))
        filtered_sample_sets.append(sample_set)
    logging.info("\tNumber of filtered sample sets: %d" % 
                 (len(filtered_sample_sets)))
    sample_sets = filtered_sample_sets
    return sample_sets

def ssea_mapreduce(config):
    '''
    multiprocessing map/reduce implementation of ssea    
    '''
    # create output directory
    if not os.path.exists(config.output_dir):
        logging.debug("Creating output directory '%s'" % (config.output_dir))
        os.makedirs(config.output_dir)
    # read/process weight matrix
    bm = BigCountMatrix.open(config.matrix_dir)
    if bm.size_factors is None:
        logging.debug("No size factors found in count matrix.. estimating")
        bm.estimate_size_factors('deseq')
    nrows = bm.shape[0]
    bm.close()
    # read/process sample sets
    sample_sets = read_sample_sets(config)
    for i,sample_set in enumerate(sample_sets):
        logging.info("Sample Set: %s" % (sample_set.name))
        # create sample set directory
        sample_set_dir = os.path.join(config.output_dir, 'sample_set_%d' % (i))
        if not os.path.exists(sample_set_dir):
            logging.debug("Creating sample set directory '%s'" % (sample_set_dir))
            os.makedirs(sample_set_dir)
        # create temp directory
        tmp_dir = os.path.join(sample_set_dir, Config.TMP_DIR)
        if not os.path.exists(tmp_dir):
            logging.debug("Creating tmp directory '%s'" % (tmp_dir))
            os.makedirs(tmp_dir)            
        # write configuration file
        logging.debug("Writing configuration file")
        config_file = os.path.join(sample_set_dir, Config.CONFIG_JSON_FILE) 
        with open(config_file, 'w') as fp:
            print >>fp, config.to_json()
        # write sample set file
        logging.debug("Writing sample set file")
        sample_set_file = os.path.join(sample_set_dir, Config.SAMPLE_SET_JSON_FILE) 
        with open(sample_set_file, 'w') as fp:
            print >>fp, sample_set.to_json() 
        # map work into chunks
        worker_basenames = []
        worker_chunks = []
        for startrow,endrow in chunk(nrows, config.num_processes):
            i = len(worker_basenames)
            logging.debug("Worker process %d range %d-%d (%d total rows)" % 
                          (i, startrow, endrow, (endrow-startrow)))
            # worker output files
            basename = os.path.join(tmp_dir, "w%d" % (i))
            worker_basenames.append(basename)
            worker_chunks.append((startrow, endrow))
        # output files
        logging.info("Running SSEA map step with %d parallel processes " % 
                     (len(worker_basenames)))
        # map step
        ssea_map(config, sample_set, worker_basenames, worker_chunks)
        # reduce step
        logging.info("Computing FDR q values")
        hists_file = os.path.join(sample_set_dir, Config.OUTPUT_HISTS_FILE)
        json_file = os.path.join(sample_set_dir, Config.RESULTS_JSON_FILE)
        ssea_reduce(worker_basenames, json_file, hists_file)
        # cleanup
        if os.path.exists(tmp_dir):
            shutil.rmtree(tmp_dir)
        logging.debug("Sample Set '%s' Done." % (sample_set.name))
    return 0

def ssea_cluster_init(config, ssea_script):
    '''
    cluster implementation of ssea
    
    call after initializing an output directory using init
    '''
    def qsub(lines):
        '''function to submit a job using qsub'''
        p = subprocess.Popen("qsub", stdin=subprocess.PIPE, stdout=subprocess.PIPE)
        p.stdin.write('\n'.join(lines))
        job_id = p.communicate()[0]
        return job_id.strip()    
    # read pbs configuration commands
    pbs_script_lines = []
    if config.pbs_script is not None:
        with open(config.pbs_script) as f:
            pbs_script_lines.extend(line.strip() for line in f)
    # read/process weight matrix
    bm = BigCountMatrix.open(config.matrix_dir)
    if bm.size_factors is None:
        logging.debug("No size factors found in count matrix.. estimating")
        bm.estimate_size_factors('deseq')
    shape = bm.shape    
    bm.close()
    # read/process sample sets
    sample_sets = read_sample_sets(config)
    for i,sample_set in enumerate(sample_sets):
        logging.info("Sample Set: %s" % (sample_set.name))
        # create sample set directory
        sample_set_dir = os.path.join(config.output_dir, 'sample_set_%d' % (i))
        if not os.path.exists(sample_set_dir):
            logging.debug("Creating sample set directory '%s'" % (sample_set_dir))
            os.makedirs(sample_set_dir)
        # create temp directory
        tmp_dir = os.path.join(sample_set_dir, Config.TMP_DIR)
        if not os.path.exists(tmp_dir):
            logging.debug("Creating tmp directory '%s'" % (tmp_dir))
            os.makedirs(tmp_dir)
        # create log directory
        log_dir = os.path.join(sample_set_dir, Config.LOG_DIR)
        if not os.path.exists(log_dir):
            logging.debug("Creating log directory '%s'" % (log_dir))
            os.makedirs(log_dir)
        # write configuration file
        logging.debug("Writing configuration file")
        config_file = os.path.join(sample_set_dir, Config.CONFIG_JSON_FILE) 
        with open(config_file, 'w') as fp:
            print >>fp, config.to_json()
        # write sample set file
        logging.debug("Writing sample set file")
        sample_set_file = os.path.join(sample_set_dir, Config.SAMPLE_SET_JSON_FILE) 
        with open(sample_set_file, 'w') as fp:
            print >>fp, sample_set.to_json()
        # map work into chunks
        worker_file = os.path.join(sample_set_dir, CLUSTER_WORKER_FILE)
        num_workers = 0
        max_chunk_size = 0
        with open(worker_file, 'w') as f:
            for startrow, endrow in chunk(shape[0], config.num_processes):
                i = num_workers
                logging.debug("Worker process %d range %d-%d (%d total rows)" % 
                              (i, startrow, endrow, (endrow-startrow)))
                basename = os.path.join(tmp_dir, "w%d" % (i))
                fields = [i, basename, startrow, endrow]
                print >>f, '\t'.join(map(str,fields))
                max_chunk_size = max(max_chunk_size, (endrow - startrow))
                num_workers += 1
        # calculate job walltime
        num_tests = (max_chunk_size * shape[1] * 
                     (1 + config.perms + config.resampling_iterations))
        hours = (num_tests * SECS_PER_ITER_MAP * HOURS_PER_SEC)
        padded_hours = int(hours + PADDING_HOURS) # padding
        logging.debug('Map step walltime calculation: %d rows/chunk %d '
                      'samples %d perms %d resamplingss' %
                      (max_chunk_size, shape[1], config.perms, 
                       config.resampling_iterations))
        logging.debug('\tAt %fs per iteration this will take %f hours' % 
                      (SECS_PER_ITER_MAP, hours))
        logging.debug('\tPadded walltime is %dh' % (padded_hours))
        # make pbs resource fields
        resource_fields = []
        resource_fields.append('walltime=%d:00:00' % (padded_hours))
        resource_fields.append('nodes=1:ppn=1')
        resource_fields.append('pmem=1024mb')
        # write map script
        lines = ['#!/bin/bash']
        lines.extend(pbs_script_lines)
        lines.append('#PBS -N %s' % (config.name))
        lines.append('#PBS -l %s' % (','.join(resource_fields)))
        lines.append('#PBS -t 0-%d' % (num_workers-1))
        lines.append('#PBS -V')
        lines.append("#PBS -o %s" % (os.path.join(log_dir, 'map.stdout')))
        lines.append("#PBS -e %s" % (os.path.join(log_dir, 'map.stderr')))
        # command line args
        args = ['python', ssea_script, '-v', '--cluster-map', '-o', 
                sample_set_dir]
        lines.append(' '.join(args))
        lines.append('')
        # write script to file
        script_file = os.path.join(config.output_dir, CLUSTER_MAP_SCRIPT)
        with open(script_file, 'w') as f:
            f.write('\n'.join(lines))
        # submit map job
        job_array_id = qsub(lines)
        logging.debug('Submitted map job array id=%s' % (job_array_id))
        # calculate walltime for reduce step
        num_results = shape[0]
        hours = (num_results * SECS_PER_ITER_REDUCE * HOURS_PER_SEC)
        padded_hours = int(hours + PADDING_HOURS) # padding
        logging.debug('Reduce walltime calculation: %d rows' % (shape[0])) 
        logging.debug('\tAt %fs per iteration this will take %f hours' % 
                      (SECS_PER_ITER_REDUCE, hours))
        logging.debug('\tPadded reduce walltime is %dh' % (padded_hours))
        # make pbs resource fields
        resource_fields = []
        resource_fields.append('walltime=%d:00:00' % (padded_hours))
        resource_fields.append('nodes=1:ppn=1')
        resource_fields.append('pmem=3750mb')
        lines = ['#!/bin/bash']
        lines.extend(pbs_script_lines)
        lines.append('#PBS -N %s' % (config.name))
        lines.append('#PBS -l %s' % (','.join(resource_fields)))
        lines.append('#PBS -V')
        lines.append("#PBS -o %s" % (os.path.join(log_dir, 'reduce.stdout')))
        lines.append("#PBS -e %s" % (os.path.join(log_dir, 'reduce.stderr')))
        lines.append("#PBS -W depend=afterokarray:%s" % (job_array_id))
        # command line args
        args = ['python', ssea_script, '-v', '--cluster-reduce', '-o', 
                sample_set_dir]
        lines.append(' '.join(args))
        lines.append('')
        # write script to file
        script_file = os.path.join(config.output_dir, CLUSTER_REDUCE_SCRIPT)
        with open(script_file, 'w') as f:
            f.write('\n'.join(lines))
        # submit reduce job
        qsub(lines)
    # cleanup
    bm.close()
    return 0

def ssea_cluster_map(output_dir):
    def _open_output_dir(output_dir):
        # read config
        config_json_file = os.path.join(output_dir, Config.CONFIG_JSON_FILE)
        config = Config.parse_json(config_json_file)
        # read sample sets
        sample_set_json_file = os.path.join(output_dir, Config.SAMPLE_SET_JSON_FILE)
        sample_set = SampleSet.parse_json(sample_set_json_file)
        return config, sample_set    
    def _read_worker_chunk(filename, index):    
        with open(filename, 'r') as f:
            # search for worker index
            for line in f:
                fields = line.strip().split('\t')
                i = int(fields[0])
                basename = fields[1]
                startrow = int(fields[2])
                endrow = int(fields[3])
                if i == index:
                    return (basename, startrow, endrow)
        logging.error("index '%d' not found in worker chunk file" % (index))
        assert False        
    # the PBS_ARRAY_INDEX environment variable should be set
    worker_index = os.environ.get('PBS_ARRAYID', None)
    if worker_index is None:
        logging.error('Could not find "PBS_ARRAYID" environment variable')
        return 1
    worker_index = int(worker_index)
    worker_file = os.path.join(output_dir, CLUSTER_WORKER_FILE)
    worker_basename, startrow, endrow = _read_worker_chunk(worker_file, worker_index)
    # open JSON files in output directory
    config, sample_set = _open_output_dir(output_dir)
    # run SSEA algorithm
    ssea_serial(config, sample_set, worker_basename, startrow, endrow)
    return 0

def ssea_cluster_reduce(output_dir):
    # read worker information
    worker_file = os.path.join(output_dir, CLUSTER_WORKER_FILE)
    with open(worker_file, 'r') as f:
        worker_basenames = [x.strip().split('\t')[1] for x in f] 
    # reduce step
    logging.info("Computing FDR q values")
    hists_file = os.path.join(output_dir, Config.OUTPUT_HISTS_FILE)
    json_file = os.path.join(output_dir, Config.RESULTS_JSON_FILE)
    # open JSON files in output directory
    ssea_reduce(worker_basenames, json_file, hists_file)
    # cleanup
    tmp_dir = os.path.join(output_dir, Config.TMP_DIR)
    if os.path.exists(tmp_dir):
        shutil.rmtree(tmp_dir)
    logging.debug("Done.")
    return 0

def main():
    # create instance of run configuration
    config = Config.parse_args()
    # show configuration
    config.log()
    # decide how to run ssea
    if config.cluster is None:
        # run on a single machine (not a cluster)
        retcode = ssea_mapreduce(config)
    else:
        if config.cluster == 'map':
            retcode = ssea_cluster_map(config.output_dir)
        elif config.cluster == 'reduce':
            retcode = ssea_cluster_reduce(config.output_dir)
        else:
            # set absolute path to this script
            ssea_script = os.path.abspath(sys.argv[0])
            # cluster setup script
            retcode = ssea_cluster_init(config, ssea_script)
    return retcode

if __name__ == "__main__":
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = '_profile.bin'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit(main())