#!/bin/env python2.7
# encoding: utf-8
'''
 -- Sample Set Enrichment Analysis (SSEA) --

Assessment of enrichment in a ranked list of quantitative measurements 

@author:     mkiyer
@author:     yniknafs
        
@copyright:  2013 Michigan Center for Translational Pathology. All rights reserved.
        
@license:    GPL2

@contact:    mkiyer@umich.edu
@deffield    updated: Updated
'''
import sys
import os
import argparse
import logging
import itertools

__all__ = []

DEBUG = 1
PROFILE = 1

# local imports
from ssea import __version__, __date__, __updated__
from ssea.base import SampleSet, Config, Metadata
from ssea.countdata import BigCountMatrix
from ssea.algo import ssea_main

# default command line options
DEFAULT_SMIN = 1
DEFAULT_SMAX = 0
DEFAULT_NA_VALUE = 'NA'

def main(argv=None):
    '''Command line options.'''    
    if argv is None:
        argv = sys.argv
    else:
        sys.argv.extend(argv)

    program_name = os.path.basename(sys.argv[0])
    program_version = "v%s" % __version__
    program_build_date = str(__updated__)
    program_version_message = '%s %s (%s)' % (program_name, program_version, program_build_date)
    program_shortdesc = __import__('__main__').__doc__.split("\n")[1]
    program_license = '''%s

  Created by mkiyer and yniknafs on %s.
  Copyright 2013 MCTP. All rights reserved.
  
  Licensed under the GPL
  http://www.gnu.org/licenses/gpl.html
  
  Distributed on an "AS IS" basis without warranties
  or conditions of any kind, either express or implied.

USAGE
''' % (program_shortdesc, str(__date__))

    # create instance of run configuration
    config = Config()
    # Setup argument parser
    parser = argparse.ArgumentParser(description=program_license)
    # Add command line parameters
    config.update_argument_parser(parser)
    parser.add_argument("-v", "--verbose", dest="verbose", 
                        action="store_true", default=False, 
                        help="set verbosity level [default: %(default)s]")
    parser.add_argument('-V', '--version', action='version', 
                        version=program_version_message)
    parser.add_argument('--smin', dest="smin", type=int,
                        default=DEFAULT_SMIN, metavar="N",
                        help='Exclude sample sets smaller than N '
                        'from the analysis [default=%(default)s]')
    parser.add_argument('--smax', dest="smax", type=int,
                        default=DEFAULT_SMAX, metavar="N",
                        help='Exclude sample sets larger than N '
                        'from the analysis [default=%(default)s]')
    parser.add_argument('--smx', dest="smx_files", action='append',
                        help='File(s) containing sets in column format')
    parser.add_argument('--smt', dest="smt_files", action='append',
                        help='File(s) containing sets in row format')
    parser.add_argument('--colmeta', dest='col_metadata_file',
                        help='file containing metadata corresponding to each '
                        'column of the weight matrix file')
    parser.add_argument('--rowmeta', dest='row_metadata_file',
                        help='file containing metadata corresponding to each '
                        'row of the weight matrix file')
    parser.add_argument('--na-value', dest='na_values', 
                        default=[DEFAULT_NA_VALUE], action='append',
                        help='Value to interpret as missing/invalid '
                        'in weight matrix [default=%(default)s]')  
    # Process arguments
    args = parser.parse_args()
    # setup logging
    if DEBUG or (args.verbose > 0):
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(level=level,
                        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    # initialize configuration
    config.parse_args(parser, args)
    # sample set size limits
    smin = max(1, args.smin)
    smax = max(0, args.smax)
    # check metadata
    if args.row_metadata_file is not None:
        if not os.path.exists(args.row_metadata_file):
            parser.error('row metadata file "%s" not found' % (args.row_metadata_file))
    if args.col_metadata_file is not None:
        if not os.path.exists(args.col_metadata_file):
            parser.error('col metadata file "%s" not found' % (args.col_metadata_file))
    # check sample sets
    smx_files = []
    if args.smx_files is not None:
        for filename in args.smx_files:
            if not os.path.exists(filename):
                parser.error("smx file '%s' not found" % (filename))
            smx_files.append(filename)
    smt_files = []
    if args.smt_files is not None:
        for filename in args.smt_files:
            if not os.path.exists(filename):
                parser.error("smt file '%s' not found" % (filename))
            smt_files.append(filename)
    if len(smx_files) == 0 and len(smt_files) == 0:
        parser.error("No sample sets specified")
    # read/process weight matrix
    if args.tsv_file is not None:
        logging.info("Converting text matrix file to binary format")
        if not os.path.exists(config.matrix_dir):
            os.makedirs(config.matrix_dir)
        bm = BigCountMatrix.from_tsv(args.tsv_file, 
                                     config.matrix_dir, 
                                     na_values=args.na_values)
        # TODO: add parameter
        bm.estimate_size_factors('deseq')
    else:
        bm = BigCountMatrix.open(args.matrix_dir)
        if bm.size_factors is None:
            bm.estimate_size_factors('deseq')
    # read metadata
    if args.row_metadata_file is not None:
        logging.info("Reading row metadata")
        row_metadata = Metadata.parse_tsv(args.row_metadata_file)
    else:
        id_iter = itertools.count()
        row_metadata = [Metadata(id_iter.next(), x) for x in bm.rownames]
    if args.col_metadata_file is not None:
        logging.info("Reading column metadata")
        col_metadata = Metadata.parse_tsv(args.col_metadata_file)
    else:
        id_iter = itertools.count()
        col_metadata = [Metadata(id_iter.next(), x) for x in bm.colnames]
    # setup output directory
    if not os.path.exists(config.output_dir):
        logging.info("Creating output directory '%s'" % 
                     (config.output_dir))
        os.makedirs(config.output_dir)    
    # read sample sets
    logging.info("Reading sample sets")
    sample_sets = []
    for filename in smx_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smx(filename, col_metadata))
    for filename in smt_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smt(filename, col_metadata))
    logging.info("\tNumber of sample sets: %d" % (len(sample_sets)))
    filtered_sample_sets = []
    for sample_set in sample_sets:
        if ((smin > 0) and (len(sample_set) < smin)):
            logging.warning("\tsample set %s excluded because size %d < %d" %                              
                            (sample_set.name, len(sample_set), smin))
            continue        
        if ((smax > 0) and (len(sample_set) > smax)):
            logging.warning("\tsample set %s excluded because size %d > %d" % 
                            (sample_set.name, len(sample_set), smax))
            continue
        logging.debug("\tsample set %s size %d" % 
                      (sample_set.name, len(sample_set)))
        filtered_sample_sets.append(sample_set)
    logging.info("\tNumber of filtered sample sets: %d" % 
                 (len(filtered_sample_sets)))
    # TODO: remove duplicate sample sets
    sample_sets = filtered_sample_sets
    # re-assign contiguous identifiers
    for i,sample_set in enumerate(sample_sets):
        sample_set._id = i
    # show final configuration
    config.log()
    # run ssea
    ssea_main(config, sample_sets, row_metadata, col_metadata)
    return 0

if __name__ == "__main__":
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = '_profile.bin'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit(main())