#!/bin/env python2.7
# encoding: utf-8
'''
 -- Sample Set Enrichment Analysis (SSEA) --

Assessment of enrichment in a ranked list of quantitative measurements 

@author:     mkiyer
@author:     yniknafs
        
@copyright:  2013 Michigan Center for Translational Pathology. All rights reserved.
        
@license:    GPL2

@contact:    mkiyer@umich.edu
@deffield    updated: Updated
'''
# set matplotlib backend
import matplotlib
matplotlib.use('Agg')

import sys
import os
import shutil
import argparse
import logging
import json
import matplotlib.pyplot as plt
from multiprocessing import Process, JoinableQueue

__all__ = []

DEBUG = 1
TESTRUN = 0
PROFILE = 0

# setup path to web files
import ssea as _ssea
SRC_WEB_PATH = os.path.join(_ssea.__path__[0], 'web')

# local imports
from ssea import __version__, __date__, __updated__
from ssea.config import Config, DETAILS_DIR
from ssea.base import SampleSet, WeightVector
from ssea.algo import ssea_run

# setup html template environment
from jinja2 import Environment, PackageLoader
env = Environment(loader=PackageLoader("ssea", "templates"),
                  extensions=["jinja2.ext.loopcontrols"])

# header fields for report
REPORT_FIELDS = ['name', 
                 'desc',
                 'sample_set_name',
                 'sample_set_desc',
                 'sample_set_size',
                 'es',
                 'nes',
                 'nominal_p_value',
                 'fdr_q_value',
                 'fwer_p_value',
                 'rank_at_max',
                 'leading_edge_num_hits',
                 'leading_edge_frac_hits',
                 'sample_set_frac_in_leading_edge',
                 'null_set_frac_in_leading_edge',
                 'details']

# matplotlib static figure
global_fig = plt.figure(0)

def format_result(name, desc, res):
    # calculate leading edge stats
    member_inds = (res.membership > 0).nonzero()[0]
    le_num_hits = sum(ind <= res.es_run_ind 
                      for ind in member_inds)
    le_num_misses = res.es_run_ind - le_num_hits
    num_misses = res.membership.shape[0] - len(res.sample_set)
    sample_set_frac_le = float(le_num_hits) / len(res.sample_set)
    null_set_frac_le = float(le_num_misses) / num_misses
    if res.es_run_ind == 0:
        le_frac_hits = 0.0
    else:
        le_frac_hits = float(le_num_hits) / res.es_run_ind
    # write result to text file            
    fields = [name, desc,
              res.sample_set.name, res.sample_set.desc, 
              len(res.sample_set), res.es, res.nes, res.pval, 
              res.qval, res.fwerp, res.es_run_ind,
              le_num_hits, le_frac_hits, 
              sample_set_frac_le,
              null_set_frac_le]
    return fields

def write_details(name, desc, res, config):
    '''
    name: string
    desc: string
    res: algo.SampleSetResult object
    config: config.Config object
    
    returns dict containing filenames written
    '''
    d = {}
    if config.create_plots:
        # create enrichment plot
        res.plot(plot_conf_int=config.plot_conf_int,
                 conf_int=config.conf_int, fig=global_fig)    
        # save plots
        eplot_png = '%s.%s.eplot.png' % (name, res.sample_set.name)
        eplot_pdf = '%s.%s.eplot.pdf' % (name, res.sample_set.name)
        global_fig.savefig(os.path.join(config.details_dir, eplot_png))
        global_fig.savefig(os.path.join(config.details_dir, eplot_pdf))
        # create null distribution plot
        res.plot_null_distribution(fig=global_fig)
        nplot_png = '%s.%s.null.png' % (name, res.sample_set.name)
        nplot_pdf = '%s.%s.null.pdf' % (name, res.sample_set.name)
        global_fig.savefig(os.path.join(config.details_dir, nplot_png))        
        global_fig.savefig(os.path.join(config.details_dir, nplot_pdf))
        d.update({'eplot_png': eplot_png,
                  'nplot_png': nplot_png})
        d.update({'eplot_pdf': eplot_pdf,
                  'nplot_pdf': nplot_pdf})
    # write detailed report
    details_rows = res.get_details_table()
    details_tsv = '%s.%s.tsv' % (name, res.sample_set.name)
    fp = open(os.path.join(config.details_dir, details_tsv), 'w')
    for fields in details_rows:
        print >>fp, '\t'.join(map(str,fields))
    fp.close()
    d['tsv'] = details_tsv
    # render to html
    if config.create_html:
        result_dict = dict(zip(REPORT_FIELDS, format_result(name, desc, res)))
        details_html = '%s.%s.html' % (name, res.sample_set.name)
        t = env.get_template('details.html')
        fp = open(os.path.join(config.details_dir, details_html), 'w')
        print >>fp, t.render(res=result_dict, 
                             files=d,
                             details=details_rows)
        fp.close()
        d['html'] = details_html
    return d

def write_html_report(filename, config):
    def parse_report_as_dicts(filename):
        '''
        parses lines of the out.txt report file produced by SSEA and 
        generates dictionaries using the first line of the file
        containing the header fields
        '''
        fileh = open(filename, 'r')
        header_fields = fileh.next().strip().split('\t')
        details_ind = header_fields.index('details')
        for line in fileh:
            fields = line.strip().split('\t')
            fields[details_ind] = json.loads(fields[details_ind])
            yield dict(zip(header_fields, fields))
        fileh.close()
    report_html = 'out.html'
    t = env.get_template('report.html')
    fp = open(os.path.join(config.output_dir, report_html), 'w')
    print >>fp, t.render(name=config.name,
                         details_dir=DETAILS_DIR,
                         results=parse_report_as_dicts(filename))
    fp.close()

def ssea_worker(input_queue, worker_tsv_file, samples, sample_sets, config):
    # setup output file
    fp = open(worker_tsv_file, 'w')
    try:
        # process input
        while True:
            name, desc, weights = input_queue.get()
            if (name is None):
                break
            results = ssea_run(samples, weights, sample_sets, 
                               weight_method_miss=config.weight_miss,
                               weight_method_hit=config.weight_hit,
                               perms=config.perms)
            for res in results:
                # get output fields
                fields = format_result(name, desc, res)
                # decide whether to create detailed report
                if res.qval <= config.fdr_qval_threshold:                
                    details_dict = write_details(name, desc, res, config)
                    details_json = json.dumps(details_dict)
                else:
                    details_json = json.dumps({})
                # output to text file
                fields.append(details_json)
                print >>fp, '\t'.join(map(str, fields))
            input_queue.task_done()
    except:
        logging.error('Exception in ssea_worker')
        input_queue.cancel_join_thread()
        raise
    # cleanup output files
    fp.close()
    input_queue.task_done()

def ssea_main(config):
    # read sample sets
    logging.info("Reading sample sets")
    sample_sets = []
    for filename in config.smx_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smx(filename))
    for filename in config.smt_files:
        logging.debug("\tFile: %s" % (filename))
        sample_sets.extend(SampleSet.parse_smt(filename))
    logging.info("\tNumber of sample sets: %d" % (len(sample_sets)))
    filtered_sample_sets = []
    for sample_set in sample_sets:
        if ((config.sample_set_size_min > 0) and
            (len(sample_set.value) < config.sample_set_size_min)):
            logging.warning("\tsample set %s excluded because size %d < %d" %                              
                            (sample_set.name, len(sample_set.value), 
                             config.sample_set_size_min))
            continue        
        if ((config.sample_set_size_max > 0) and 
            (len(sample_set.value) > config.sample_set_size_max)):
            logging.warning("\tsample set %s excluded because size %d > %d" % 
                            (sample_set.name, len(sample_set.value), 
                             config.sample_set_size_max))
            continue
        logging.debug("\tsample set %s size %d" % (sample_set.name, len(sample_set)))
        filtered_sample_sets.append(sample_set)
    logging.info("\tNumber of filtered sample sets: %d" % (len(filtered_sample_sets)))
    sample_sets = filtered_sample_sets
    # setup output directory
    if not os.path.exists(config.output_dir):
        logging.info("Creating output directory '%s'" % 
                     (config.output_dir))
        os.makedirs(config.output_dir)
    if not os.path.exists(config.details_dir):
        logging.debug("\tCreating details directory '%s'" % 
                      (config.details_dir))
        os.makedirs(config.details_dir)
    # create temp directory
    tmp_dir = os.path.join(config.output_dir, "tmp")
    if not os.path.exists(tmp_dir):
        logging.debug("\tCreating tmp directory '%s'" % (tmp_dir))
        os.makedirs(tmp_dir)
    # create directory for static web files (CSS, javascript, etc)
    web_dir = os.path.join(config.output_dir, 'web')
    if not os.path.exists(web_dir):
        logging.info("\tInstalling web files")
        shutil.copytree(SRC_WEB_PATH, web_dir)
    # inspect weight vector file to get samples
    wmt_fileh = open(config.weight_matrix_file)
    wmt_header_fields = wmt_fileh.next().strip().split('\t')
    samples = wmt_header_fields[WeightVector.METADATA_COLS:]
    # create multiprocessing queue for passing data
    input_queue = JoinableQueue(maxsize=config.num_processors*3)
    # start worker processes
    logging.info("Running SSEA")
    procs = []
    worker_tsv_files = []
    try:
        for i in xrange(config.num_processors):
            worker_tsv_file = os.path.join(tmp_dir, "worker%03d.tsv" % (i))
            worker_tsv_files.append(worker_tsv_file)
            args = (input_queue, worker_tsv_file, samples, sample_sets, config)
            p = Process(target=ssea_worker, args=args)
            p.daemon = True
            p.start()
            procs.append(p)
        # parse wmt file
        for line in wmt_fileh:
            fields = line.strip().split('\t')
            name = fields[0]
            desc = fields[1]
            weights = map(float,fields[2:])
            logging.info("\tName: %s (%s)" % (name, desc))
            input_queue.put((name, desc, weights))
    finally:
        # stop workers
        for p in procs:
            input_queue.put((None, None, None))
        # close queue
        input_queue.join()
        input_queue.close()
        # join worker processes
        for p in procs:
            p.join()
    wmt_fileh.close()
    # merge output files
    logging.info("Merging %d worker report files" % 
                 (config.num_processors))
    report_tsv_file = os.path.join(config.output_dir, 'out.tsv')
    fp = open(report_tsv_file, 'w')
    print >>fp, '\t'.join(REPORT_FIELDS)
    for filename in worker_tsv_files:
        shutil.copyfileobj(open(filename), fp)
    fp.close()
    # remove worker files
    for filename in worker_tsv_files:
        if os.path.exists(filename):
            os.remove(filename)
    # cleanup
    if os.path.exists(tmp_dir):
        shutil.rmtree(tmp_dir)
    # create html report
    if config.create_html:
        logging.info("Writing HTML Report")
        write_html_report(report_tsv_file, config)
    # free resources
    plt.close('all')
    logging.info("Finished")

class CLIError(Exception):
    '''Generic exception to raise and log different fatal errors.'''
    def __init__(self, msg):
        super(CLIError).__init__(type(self))
        self.msg = "ERROR: %s" % msg
    def __str__(self):
        return self.msg
    def __unicode__(self):
        return self.msg

def main(argv=None):
    '''Command line options.'''    
    if argv is None:
        argv = sys.argv
    else:
        sys.argv.extend(argv)

    program_name = os.path.basename(sys.argv[0])
    program_version = "v%s" % __version__
    program_build_date = str(__updated__)
    program_version_message = '%s %s (%s)' % (program_name, program_version, program_build_date)
    program_shortdesc = __import__('__main__').__doc__.split("\n")[1]
    program_license = '''%s

  Created by mkiyer and yniknafs on %s.
  Copyright 2013 MCTP. All rights reserved.
  
  Licensed under the GPL
  http://www.gnu.org/licenses/gpl.html
  
  Distributed on an "AS IS" basis without warranties
  or conditions of any kind, either express or implied.

USAGE
''' % (program_shortdesc, str(__date__))

    # create instance of run configuration
    config = Config()
    config.version = program_version_message
    try:
        # Setup argument parser
        parser = argparse.ArgumentParser(description=program_license)
        # Add command line parameters
        config.get_argument_parser(parser)
        parser.add_argument("-v", "--verbose", dest="verbose", 
                            action="store_true", default=False, 
                            help="set verbosity level [default: %(default)s]")
        parser.add_argument('-V', '--version', action='version', 
                            version=program_version_message)
        # Process arguments
        args = parser.parse_args()
        # setup logging
        if DEBUG or (args.verbose > 0):
            level = logging.DEBUG
        else:
            level = logging.INFO
        logging.basicConfig(level=level,
                            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        # initialize configuration
        config.parse_args(parser, args)
        config.log()
        # run
        ssea_main(config)
    except KeyboardInterrupt:
        ### handle keyboard interrupt ###
        pass
#     except Exception, e:
#         pass
#         if DEBUG or TESTRUN:
#             raise(e)
#         indent = len(program_name) * " "
#         logging.error(program_name + ": " + repr(e) + "\n")
#         logging.error(indent + "  for help use --help")
#         return 2
    return 0

if __name__ == "__main__":
    if DEBUG:
        pass
    if TESTRUN:
        pass
        #import doctest
        #doctest.testmod()
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = '_profile.bin'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit(main())