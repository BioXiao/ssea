#!/bin/env python2.7
# encoding: utf-8
'''
Created on Oct 29, 2013

@author: mkiyer
'''
import sys
import os
import argparse
import logging
import shutil
import re
from multiprocessing import Process, Queue

# set matplotlib backend
import matplotlib
matplotlib.use('Agg')

# third-party packages
import numpy as np
from jinja2 import Environment, PackageLoader
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import matplotlib.cm as cm
from matplotlib import figure

# local imports
from ssea.lib.kernel import ssea_kernel, RandomState
from ssea.lib.base import INT_DTYPE, Result, Metadata, SampleSet
from ssea.lib.config import Config, timestamp
from ssea.lib.countdata import BigCountMatrix

# setup path to web files
import ssea
SRC_WEB_PATH = os.path.join(ssea.__path__[0], 'web')

# setup html template environment
env = Environment(loader=PackageLoader("ssea", "templates"),
                  extensions=["jinja2.ext.loopcontrols"])

# matplotlib static figure for plotting
global_fig = plt.figure(0)

# functions to test operator
OP_TEST_FUNCS = {'<=': lambda a,b: a<=b,
                 '>=': lambda a,b: a>=b,
                 '<': lambda a,b: a<b,
                 '>': lambda a,b: a>b}

REPORT_DIR = 'report'

class ReportConfig(object):
    def __init__(self):
        self.num_processes = 1
        self.input_dir = None
        self.thresholds = []
        self.create_html = True
        self.create_pdf = True

    def get_argument_parser(self, parser=None):
        if parser is None:
            parser = argparse.ArgumentParser()
        grp = parser.add_argument_group("SSEA Report Options")
        grp.add_argument('-p', '--num-processes', dest='num_processes',
                         type=int, default=1,
                         help='Number of processor cores available '
                         '[default=%(default)s]')
        grp.add_argument('--no-html', dest="create_html", 
                         action="store_false", default=self.create_html,
                         help='Do not create detailed html reports')
        grp.add_argument('--no-pdf', dest="create_pdf", 
                         action="store_false", default=self.create_pdf,
                         help='Do not create PDF plots')
        grp.add_argument('-t', '--threshold', action="append", 
                         dest="thresholds",
                         help='Significance thresholds for generating '
                         'detailed reports [default=%(default)s]')
        grp.add_argument('--colmeta', dest='col_metadata_file',
                         help='file containing metadata corresponding to each '
                         'column of the weight matrix file')
        grp.add_argument('--rowmeta', dest='row_metadata_file',
                         help='file containing metadata corresponding to each '
                         'row of the weight matrix file')
        grp.add_argument('input_dir')
        return parser        

    def log(self, log_func=logging.info):
        log_func("Parameters")
        log_func("----------------------------------")
        log_func("num processes:           %d" % (self.num_processes))
        log_func("input directory:         %s" % (self.input_dir))
        log_func("create html report:      %s" % (self.create_html))
        log_func("create pdf plots:        %s" % (self.create_pdf))
        log_func("thresholds:              %s" % (','.join(''.join(map(str,t)) for t in self.thresholds)))
        log_func("----------------------------------")

    def parse_args(self, parser, args):
        # process and check arguments
        self.create_html = args.create_html
        self.create_pdf = args.create_pdf
        self.num_processes = args.num_processes
        # parse threshold arguments of the form 'attribute,value'
        # for example: nominal_p_value,0.05
        if args.thresholds is not None:
            for arg in args.thresholds:
                m = re.match(r'(.+)([<>]=?)(.+)', arg)
                if m is None:
                    parser.error('error parsing threshold argument "%s"' % (arg))
                attr, op, value = m.groups()
                if attr not in Result.FIELDS:
                    parser.error('threshold attribute "%s" unknown' % (attr))
                if op not in OP_TEST_FUNCS:
                    parser.error('unrecognized operator "%s"' % (op))
                value = float(value)
                self.thresholds.append((attr,op,value))       
        # check input directory
        if not os.path.exists(args.input_dir):
            parser.error("input directory '%s' not found" % (args.input_dir))
        self.input_dir = args.input_dir
        # setup output directory
        self.output_dir = os.path.join(self.input_dir, REPORT_DIR)
        if os.path.exists(self.output_dir):
            parser.error("Report directory '%s' already exists" % 
                         (self.output_dir))
        if not os.path.exists(args.col_metadata_file):
            parser.error("Column metadata file '%s' not found" % (args.col_metadata_file))
        if not os.path.exists(args.row_metadata_file):
            parser.error("Row metadata file '%s' not found" % (args.row_metadata_file))
        self.col_metadata_file = os.path.abspath(args.col_metadata_file)
        self.row_metadata_file = os.path.abspath(args.row_metadata_file)


class SSEAData:
    def get_details_table(self, sample_metadata):
        # show details of hits
        rows = [['index', 'sample', 'rank', 'raw_weights', 'transformed_weights',
                 'running_es', 'core_enrichment']]
        for i,ind in enumerate(self.hit_indexes):
            if self.es < 0:
                is_enriched = int(ind >= self.es_rank)
            else:                
                is_enriched = int(ind <= self.es_rank)
            meta = sample_metadata[self.sample_inds[ind]]
            rows.append([i, meta.name, ind+1, self.raw_weights[ind],
                         self.weights_hit[ind], self.es_run[ind], 
                         is_enriched])
        return rows

def ssea_rerun(result, bm, sample_set, row_metadata, col_metadata, 
               reportconfig, sseaconfig):
    # get membership array for sample set
    membership = sample_set.get_array(bm.colnames)
    valid_samples = (membership >= 0)
    # read from memmap
    counts = np.array(bm.counts[result.t_id,:], dtype=np.float)
    # remove 'nan' values
    valid_inds = np.logical_and(valid_samples, np.isfinite(counts))
    # subset counts, size_factors, and membership array
    counts = counts[valid_inds]
    size_factors = bm.size_factors[valid_inds]
    valid_membership = membership[valid_inds]
    # reproduce previous run
    rng = RandomState(result.rand_seed)
    (ranks, norm_counts, norm_counts_miss, norm_counts_hit, 
     es_val, es_rank, es_run) = \
        ssea_kernel(counts, size_factors, valid_membership, rng,
                     resample_counts=False,
                     permute_samples=False,
                     add_noise=True,
                     noise_loc=sseaconfig.noise_loc, 
                     noise_scale=sseaconfig.noise_scale,
                     method_miss=sseaconfig.weight_miss,
                     method_hit=sseaconfig.weight_hit,
                     method_param=sseaconfig.weight_param)
    # make object for plotting
    m = membership[ranks]
    hit_indexes = (m > 0).nonzero()[0]
    d = SSEAData()
    d.es = es_val
    d.es_rank = es_rank
    d.es_run = es_run
    d.hit_indexes = hit_indexes
    d.ranks = ranks
    d.sample_inds = valid_inds.nonzero()[0][ranks]
    d.raw_weights = norm_counts[ranks]
    d.weights_miss = norm_counts_miss[ranks]
    d.weights_hit = norm_counts_hit[ranks]
    return d

def plot_enrichment(result, sseadata,
                    title, fig=None):
    if fig is None:
        fig = plt.Figure()
    else:
        fig.clf()
    gs = gridspec.GridSpec(3, 1, height_ratios=[2,1,1])
    # running enrichment score
    ax0 = fig.add_subplot(gs[0])
    x = np.arange(len(sseadata.es_run))
    y = sseadata.es_run
    p1 = ax0.scatter(result.resample_es_ranks, result.resample_es_vals,
                     c='r', s=25.0, alpha=0.3, edgecolors='none')
    p2 = ax0.scatter(result.null_es_ranks, result.null_es_vals,
                     c='b', s=25.0, alpha=0.3, edgecolors='none')
    ax0.plot(x, y, lw=2, color='k', label='Enrichment profile')
    ax0.axhline(y=0, color='gray')
    ax0.axvline(x=sseadata.es_rank, lw=1, linestyle='--', color='black')
    ax0.set_xlim((0, len(sseadata.es_run)))
    #ax0.set_ylim((-1.0, 1.0))
    ax0.grid(True)
    ax0.set_xticklabels([])
    ax0.set_ylabel('Enrichment score (ES)')
    ax0.set_title(title)
    legend = ax0.legend((p1,p2), ('Resampled ES', 'Null ES'), 'upper right',
                        numpoints=1, scatterpoints=1, 
                        prop={'size': 'xx-small'})
    # The frame is matplotlib.patches.Rectangle instance surrounding the legend.
    frame = legend.get_frame()
    frame.set_linewidth(0)
    # membership in sample set
    ax1 = fig.add_subplot(gs[1])
    ax1.vlines(sseadata.hit_indexes, ymin=0, ymax=1, lw=0.25, 
               color='black', label='Hits')
    ax1.set_xlim((0, len(sseadata.es_run)))
    ax1.set_ylim((0, 1))
    ax1.set_xticks([])
    ax1.set_yticks([])
    ax1.set_xticklabels([])
    ax1.set_yticklabels([])
    ax1.set_ylabel('Set')
    # weights
    ax2 = fig.add_subplot(gs[2])
    # TODO: if hit and miss weights differ add a legend
    ax2.plot(sseadata.weights_miss, color='blue')
    ax2.plot(sseadata.weights_hit, color='red')
    #ax2.plot(weights_hit, color='red')
    ax2.set_xlim((0, len(sseadata.es_run)))
    ax2.set_xlabel('Samples')
    ax2.set_ylabel('Weights')
    # draw
    fig.tight_layout()
    return fig

def plot_null_distribution(es, es_null_bins, null_es_hist, 
                           fig=None):
    if fig is None:
        fig = plt.Figure()
    else:
        fig.clf()
    # get coords of bars    
    left = es_null_bins[:-1]
    height = null_es_hist
    width = [(r-l) for l,r in zip(es_null_bins[:-1],es_null_bins[1:])]
    # make plot
    ax = fig.add_subplot(1,1,1)
    ax.bar(left, height, width)
    ax.axvline(x=es, linestyle='--', color='black')
    ax.set_title('Random ES distribution')
    ax.set_ylabel('P(ES)')
    # calculate percent neg
    percent_neg = 100.0 * sum(null_es_hist[i] for i in xrange(len(null_es_hist))
                              if es_null_bins[i] < 0)
    percent_neg /= float(sum(null_es_hist))
    ax.set_xlabel('ES (Sets with neg scores: %.1f%%)' % 
                  (percent_neg))
    return fig

def create_detailed_report(result, sseadata, rowmeta, colmeta, sample_set, 
                           reportconfig):
    '''
    Generate detailed report files including enrichment plots and 
    a tab-delimited text file showing the running ES score and rank
    of each member in the sample set
     
    returns dict containing files written
    '''    
    d = {}
    # enrichment plot
    title = 'Enrichment plot: %s vs. %s' % (rowmeta.name, sample_set.name)
    fig = plot_enrichment(result, sseadata, 
                          title=title, 
                          fig=global_fig)
    eplot_png = '%s.%s.eplot.png' % (rowmeta.name, sample_set.name)
    fig.savefig(os.path.join(reportconfig.output_dir, eplot_png))
    d['eplot_png'] = eplot_png
    if reportconfig.create_pdf:    
        eplot_pdf = '%s.%s.eplot.pdf' % (rowmeta.name, sample_set.name)
        fig.savefig(os.path.join(reportconfig.output_dir, eplot_pdf))
        d['eplot_pdf'] = eplot_pdf
    # null distribution plot
    fig = plot_null_distribution(result.es, 
                                 Config.NULL_ES_BINS,
                                 result.null_es_hist,
                                 fig=global_fig)
    nplot_png = '%s.%s.null.png' % (rowmeta.name, sample_set.name)
    fig.savefig(os.path.join(reportconfig.output_dir, nplot_png))        
    d['nplot_png'] = nplot_png
    if reportconfig.create_pdf:    
        nplot_pdf = '%s.%s.null.pdf' % (rowmeta.name, sample_set.name)
        fig.savefig(os.path.join(reportconfig.output_dir, nplot_pdf))
        d['nplot_pdf'] = nplot_pdf
    # write tab-delimited details file
    details_rows = sseadata.get_details_table(colmeta)
    details_tsv = '%s.%s.tsv' % (rowmeta.name, sample_set.name)
    fp = open(os.path.join(reportconfig.output_dir, details_tsv), 'w')
    for row in details_rows:
        print >>fp, '\t'.join(map(str,row))
    fp.close()
    d['tsv'] = details_tsv
    # render to html
    if reportconfig.create_html:
        details_html = '%s.%s.html' % (rowmeta.name, sample_set.name)
        t = env.get_template('details.html')
        fp = open(os.path.join(reportconfig.output_dir, details_html), 'w')
        print >>fp, t.render(result=result,
                             sseadata=sseadata,
                             details=details_rows,
                             files=d,
                             rowmeta=rowmeta,
                             sample_set=sample_set)
        fp.close()
        d['html'] = details_html
    return d

def report_serial(output_file, row_metadata, col_metadata, sample_set, 
                  sseaconfig, reportconfig):
    # open data matrix
    bm = BigCountMatrix.open(sseaconfig.matrix_dir)
    fout = open(output_file, 'w')
    # parse report json    
    logging.info("Processing results")
    json_file = os.path.join(reportconfig.input_dir, Config.RESULTS_JSON_FILE)
    for result in parse_and_filter_results(json_file, reportconfig.thresholds):
        # rerun ssea
        sseadata = ssea_rerun(result, bm, sample_set, row_metadata, 
                              col_metadata, reportconfig, sseaconfig)
        d = create_detailed_report(result, sseadata, 
                                   row_metadata[result.t_id], col_metadata, 
                                   sample_set, reportconfig)
        # update results with location of plot files
        result.files = d
        # write result to tab-delimited text
        print >>fout, result.to_json()
    fout.close()

def parse_and_filter_results(filename, thresholds):
    passed = 0
    failed = 0    
    with open(filename, 'r') as fin:
        for line in fin:
            # load json document (one per line)
            result = Result.from_json(line.strip())      
            # apply thresholds
            skip = False
            for attr,op,threshold in thresholds:
                value = getattr(result, attr)
                if not OP_TEST_FUNCS[op](value, threshold):
                    skip = True
                    break
            if skip:
                failed += 1
                continue
            passed += 1
            yield result
    logging.info("Parsed %d results, %d passed and %d failed" %
                 (passed+failed, passed, failed))

def _producer_process(input_queue, filename, config):
    for result in parse_and_filter_results(filename, config.thresholds):
        input_queue.put(result)
    # tell consumers to stop
    for i in xrange(config.num_processes):
        input_queue.put(None)
    logging.debug("Producer finished")

def _worker_process(input_queue, output_queue, sample_set, row_metadata, 
                    col_metadata, sseaconfig, reportconfig):
    # open data matrix
    bm = BigCountMatrix.open(sseaconfig.matrix_dir)
    # process results
    while True:
        result = input_queue.get()
        if result is None:
            break
        # rerun ssea
        sseadata = ssea_rerun(result, bm, sample_set, row_metadata, 
                              col_metadata, reportconfig, sseaconfig)
        d = create_detailed_report(result, sseadata, 
                                   row_metadata[result.t_id], col_metadata, 
                                   sample_set, reportconfig)
        # update results with location of plot files
        result.files = d
        # send result back
        output_queue.put(result)
    bm.close()
    # send done signal
    output_queue.put(None)
    logging.debug("Worker finished")

def report_parallel(output_file, row_metadata, col_metadata, 
                    sample_set, runconfig, config):
    # create multiprocessing queues for passing data
    input_queue = Queue(maxsize=config.num_processes*3)
    output_queue = Queue(maxsize=config.num_processes*3)    
    # start a producer process
    logging.debug("Starting producer process and %d workers" % (config.num_processes))
    json_file = os.path.join(config.input_dir, Config.RESULTS_JSON_FILE)
    args=(input_queue, json_file, config)
    producer = Process(target=_producer_process, args=args)
    producer.start()
    # start consumer processes
    procs = []
    for i in xrange(config.num_processes):
        args = (input_queue, output_queue, sample_set, row_metadata, 
                col_metadata, runconfig, config)
        p = Process(target=_worker_process, args=args)
        p.start()
        procs.append(p)
    # get results from consumers
    num_alive = config.num_processes
    with open(output_file, 'w') as fout:
        while num_alive > 0:
            result = output_queue.get()
            if result is None:
                num_alive -= 1
                logging.debug("Main process detected worker finished, %d still alive" % (num_alive))
            else:
                # write result to tab-delimited text
                print >>fout, result.to_json()
    logging.debug("Joining all processes")
    # wait for producer to finish
    producer.join()
    # wait for consumers to finish
    for p in procs:
        p.join()

def create_html_report(input_file, output_file, row_metadata, sample_set, 
                       runconfig):
    def _result_parser(filename):
        with open(filename, 'r') as fp:
            for line in fp:
                result = Result.from_json(line.strip())
                rowmeta = row_metadata[result.t_id]
                result.sample_set_name = sample_set.name
                result.sample_set_desc = sample_set.desc
                result.sample_set_size = len(sample_set.value_dict)
                result.name = rowmeta.name
                result.params = rowmeta.params
                yield result
    # get full list of parameters in row metadata
    allparams = set()
    for r in row_metadata:
        allparams.update(r.params.keys())
    allparams = sorted(allparams)
    # render templates
    t = env.get_template('report.html')
    with open(output_file, 'w') as fp:
        print >>fp, t.render(name=runconfig.name,
                             params=allparams,
                             results=_result_parser(input_file))

def report(config):
    # create output dir
    if not os.path.exists(config.output_dir):
        logging.debug("Creating output dir '%s'" % (config.output_dir))
        os.makedirs(config.output_dir)
    # create directory for static web files (CSS, javascript, etc)
    if config.create_html:
        web_dir = os.path.join(config.output_dir, 'web')
        if not os.path.exists(web_dir):
            logging.debug("Installing web files")
            shutil.copytree(SRC_WEB_PATH, web_dir)
    # link to input files
    sample_set_json_file = os.path.join(config.input_dir,
                                        Config.SAMPLE_SET_JSON_FILE)
    sample_set = SampleSet.parse_json(sample_set_json_file)
    config_json_file = os.path.join(config.input_dir, 
                                    Config.CONFIG_JSON_FILE)
    runconfig = Config.parse_json(config_json_file)
    # load metadata
    bm = BigCountMatrix.open(runconfig.matrix_dir)
    logging.info("Reading row metadata")
    row_metadata = list(Metadata.parse_tsv(config.row_metadata_file, bm.rownames))
    logging.info("Reading column metadata")
    col_metadata = list(Metadata.parse_tsv(config.col_metadata_file, bm.colnames))
    bm.close()
    # write filtered results to output file
    filtered_results_file = os.path.join(config.output_dir, 
                                         'filtered_results.json')
    # produce detailed reports
    if config.num_processes > 1:
        logging.debug("Creating detailed reports in parallel with %d processes" % (config.num_processes))
        report_parallel(filtered_results_file, row_metadata, col_metadata, 
                        sample_set, runconfig, config)
    else:
        logging.debug("Creating detailed reports in serial")
        report_serial(filtered_results_file, row_metadata, col_metadata, 
                      sample_set, runconfig, config)
    # produce report
    if config.create_html:
        logging.debug("Writing HTML report")
        html_file = os.path.join(config.output_dir, 'filtered_results.html')
        create_html_report(filtered_results_file, html_file,
                           row_metadata, sample_set, runconfig)
    logging.debug("Done.")
    return 0

def main(argv=None):
    # Setup argument parser
    parser = argparse.ArgumentParser()
    # Add command line parameters
    config = ReportConfig()
    config.get_argument_parser(parser)
    parser.add_argument("-v", "--verbose", dest="verbose", 
                        action="store_true", default=False, 
                        help="set verbosity level [default: %(default)s]")
    # Process arguments
    args = parser.parse_args()
    # setup logging
    if args.verbose > 0:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(level=level,
                        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    # initialize configuration
    config.parse_args(parser, args)
    config.log()
    #return report_parallel(config)
    return report(config)

if __name__ == "__main__":
    sys.exit(main())


